## Spark cluster configuration for laptop/PC usage
##
image:
  registry: docker.io
  repository: bitnami/spark
  tag: 3.5.5

## Specify a imagePullPolicy
## ref: https://kubernetes.io/docs/concepts/containers/images/#updating-images
##
imagePullPolicy: IfNotPresent

## Spark master specific configuration
##
master:
  # Configure resource limits and requests for laptop usage
  resourcesPreset: small
  resources:
    limits:
      cpu: 1
      memory: 2Gi
    requests:
      cpu: 0.5
      memory: 1Gi
  
  # Use our custom configuration
  configurationConfigMap: spark-config
  
  # Set JVM and Spark options
  configOptions:
    -Dspark.ui.reverseProxy=true
    -Dspark.master.rest.enabled=true
    -Dspark.master.ui.port=8080
    -Dspark.deploy.recoveryMode=FILESYSTEM
    -Dspark.deploy.recoveryDirectory=/tmp/spark-recovery
    -Dspark.eventLog.enabled=true
    -Dspark.eventLog.dir=/tmp/spark-events
    -Dspark.history.fs.logDirectory=/tmp/spark-events
    -Xmx1g
  
  # Set up pod anti-affinity to avoid masters on the same node
  podAntiAffinityPreset: soft

## Spark worker specific configuration
##
worker:
  # Reduce number of worker replicas for laptop usage
  replicaCount: 1
  
  # Configure resource limits and requests for laptop usage
  resourcesPreset: small
  resources:
    limits:
      cpu: 1
      memory: 2Gi
    requests:
      cpu: 0.5
      memory: 1Gi
  
  # Use our custom configuration
  configurationConfigMap: spark-config
  
  # Set memory and core limits for workers
  memoryLimit: "1.5g"
  coreLimit: "1"
  
  # Set JVM and Spark options
  configOptions:
    -Dspark.worker.ui.port=8081
    -Dspark.worker.cleanup.enabled=true
    -Dspark.worker.cleanup.interval=1800
    -Dspark.worker.cleanup.appDataTtl=604800
    -Xmx1g
  
  # Enable minimal autoscaling for laptop usage
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 2
    targetCPU: 70
    targetMemory: 80

## Prometheus metrics - reduced for laptop usage
##
metrics:
  enabled: true
  # Prometheus Operator ServiceMonitor
  serviceMonitor:
    enabled: false
  # Prometheus rules for alerting - disabled for laptop usage
  prometheusRule:
    enabled: false

## Service configuration
##
service:
  type: ClusterIP
  
  # Port configurations
  ports:
    http: 80
    https: 443
    cluster: 7077

## Ingress configuration - simplified for laptop usage
##
ingress:
  enabled: true
  hostname: spark.local
  path: /
  pathType: Prefix
  annotations:
    kubernetes.io/ingress.class: nginx
  tls: false

## Security configuration - simplified for laptop usage
##
security:
  # Basic security disabled for local development
  passwordsSecretName: ""
  rpc:
    authenticationEnabled: false
    encryptionEnabled: false
  ssl:
    enabled: false

## Storage for Spark History Server - reduced size for laptop
##
worker:
  extraVolumes:
    - name: spark-data
      persistentVolumeClaim:
        claimName: spark-data-pvc
  extraVolumeMounts:
    - name: spark-data
      mountPath: /tmp/spark-events

## PVC for Spark data - reduced size for laptop
##
persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 5Gi

## Additional Spark configurations for resource constraints
##
sparkConf:
  "spark.driver.memory": "1g"
  "spark.executor.memory": "1g"
  "spark.driver.cores": "1"
  "spark.executor.cores": "1"
  "spark.dynamicAllocation.enabled": "true"
  "spark.dynamicAllocation.initialExecutors": "1"
  "spark.dynamicAllocation.minExecutors": "0"
  "spark.dynamicAllocation.maxExecutors": "2"
  "spark.memory.fraction": "0.6"
  "spark.memory.storageFraction": "0.5"
  "spark.default.parallelism": "4"
  "spark.sql.shuffle.partitions": "4"
  "spark.kubernetes.executor.limit.cores": "1"
  "spark.kubernetes.executor.limit.memory": "1.5g"
  "spark.kubernetes.driver.limit.cores": "1"
  "spark.kubernetes.driver.limit.memory": "1.5g"